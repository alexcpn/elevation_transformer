

I am trying to use the Transformer model to model the ITM path loss function

For this I am giving the elevation 5 meters apart from receiver and transmitter using Nation Elevation Data NED
And also other parameters like receiver and tranmsitter height , center frequency  and distance between receiver and transmitter
these are given in other parameters

My network is like this

```
seq_length = 765
d_model = 160  # embediding size
final_size = 1 # we just want one value
num_heads = 16  
extra_feature_size = 4

pos_encoding = PositionalEncoding(d_model,seq_length)
input_features_projection = nn.Linear(extra_feature_size, d_model)
elevation_projection = nn.Linear(1, d_model)  # Add this to model
multihead_attention = nn.MultiheadAttention(d_model, num_heads, dropout=0.1,batch_first=True)
# multihead_attention = nn.TransformerEncoderLayer( 3tried this also
#     d_model=d_model,
#     nhead=num_heads,
#     dim_feedforward=256,   # or bigger
#     dropout=0.1,
#     batch_first=True,
# )
prediction_layer1 = nn.Linear(d_model, d_model)
layer_norm1 = nn.LayerNorm(d_model)
prediction_layer2 = nn.Linear(d_model, d_model*4)
layer_norm2 = nn.LayerNorm(d_model*4) 
prediction_layer3 = nn.Linear(d_model*4, final_size)

# Define the loss function
loss_function = nn.SmoothL1Loss() # since we have just regression
# We'll combine these into a simple pipeline
model = nn.Sequential(input_features_projection,elevation_projection,pos_encoding,
                      multihead_attention, prediction_layer1,
                      layer_norm1, prediction_layer2,layer_norm2,prediction_layer3)
# SGD is unstable and hence we use this
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
# Set up a learning rate scheduler that reduces the LR when the loss plateaus.
# Here, we reduce the LR by a factor of 0.5 if there's no improvement for 5 epochs.
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5, verbose=True
)
```

and forward pass is

```

def forward_pass(input_features, elevation_data, target_labels, padding_mask):
    input_features = input_features.to('cuda') # (B, 4)
    elevation_data = elevation_data.to('cuda')  # (B, 765) = (B, S)
    target_labels = target_labels.to('cuda') # (B, 1)
    padding_mask = padding_mask.to('cuda')
    # project to higher dim
    #print(f"input_features.shape {input_features.shape}") #torch.Size([25, 4])
    other_features_embed = input_features_projection(input_features) # (B, d_model)
   # print(f"other_features_embed.shape {other_features_embed.shape}") #t (B, d_model)
    elevation_embed = elevation_projection(elevation_data.unsqueeze(-1))  
    #print(f"elevation_embed.shape {elevation_embed.shape}") # (B, S, d_model)
    pos_embed = pos_encoding(elevation_embed) 
    #print(f"pos_embed.shape {pos_embed.shape}") # (B, S, d_model)
    elevation_score, _ = multihead_attention(pos_embed, pos_embed, pos_embed,key_padding_mask=padding_mask)
    #elevation_score = multihead_attention(pos_embed)# Transformer Encoder one
    # pool over S
    elevation_vector = torch.mean(elevation_score, dim=1)  # (B, d_model)
    #print   (f"Elevation vector shape: {elevation_vector.shape}")
    combined = other_features_embed + elevation_vector  # (B, d_model)
    #print(f"Combined shape: {combined.shape}")
    hidden1 = prediction_layer1(combined)  # through few linear layers
    residual1 =combined + hidden1
    hidden2 = layer_norm1(residual1) 
    hidden3 =  prediction_layer2(hidden2)      # add layer norm
    logits  = prediction_layer3(hidden3)
    # print(f"Logits shape: {logits.shape}") # (B, 1)
    logits = logits.squeeze(-1)  
    loss = loss_function(
            logits,
            target_labels
        )
    del  input_features,elevation_data,target_labels 
    gc.collect()
    torch.cuda.empty_cache()
    return logits,loss
```

the elevation data is seq_length max filled

```
class PathLossDataset(Dataset):
    def __init__(self, parquet_files, max_len=765):
        self.parquet_files = parquet_files
        self.max_len = max_len
        self.file_lengths = []  # Store the length of each file
        self.cumulative_lengths = [] #Store the cumulative length of all previous files.
        self._calculate_lengths() #calculate the lengths.

    def _calculate_lengths(self):
        cumulative_len = 0
        for file in self.parquet_files:
            df = pd.read_parquet(file)
            length = len(df)
            self.file_lengths.append(length)
            cumulative_len += length
            self.cumulative_lengths.append(cumulative_len)
       
    def __len__(self):
        return self.cumulative_lengths[-1]
    
    def __getitem__(self, idx):
        # Find which file the index belongs to
        file_index = 0
        for i, cumulative_len in enumerate(self.cumulative_lengths):
            if idx < cumulative_len:
                file_index = i
                break

        # Calculate the index within the selected file
        if file_index == 0:
            local_idx = idx
        else:
            local_idx = idx - self.cumulative_lengths[file_index - 1]

        # Load the relevant Parquet file
        df = pd.read_parquet(self.parquet_files[file_index])
        row = df.iloc[local_idx]
        extra_features = torch.tensor([
            row['dEP_FSRx_m'],
            row['center_freq'],
            row['receiver_ht_m'],
            row['accesspoint_ht_m']
        ], dtype=torch.float32)
         
        elevation_data = torch.tensor(row['elevation_data'], dtype=torch.float32)
        seq_len = elevation_data.shape[0]
        if seq_len < self.max_len:
            last_value = elevation_data[-1]
            # Create a padding tensor by repeating the last value
            padding = last_value.repeat(self.max_len - seq_len)
            elevation_data = torch.cat([elevation_data, padding])
        elif seq_len > self.max_len:
            elevation_data = elevation_data[:self.max_len] #truncate if longer.

        path_loss = torch.tensor(row['path_loss'], dtype=torch.float32)
        
          # 4. Create CORRECT mask (True = ignore)
        padding_mask = torch.zeros(self.max_len, dtype=torch.bool)  # Default: don't mask
        padding_mask[seq_len:] = True  # ‚Üê Mask padded positions
        return extra_features, elevation_data, path_loss,padding_mask
    ```
    
    the problem is whatever I do the loss plateus around 16 to 20
    
    
    ```
    24-Feb-25 16:20:10 - Training steps per epoch: 7460
24-Feb-25 16:20:10 - Validation steps per epoch: 829
24-Feb-25 16:20:15 - [Epoch=1 | Step=1/7460] loss=210.3021
24-Feb-25 16:20:15 - [Epoch=1 | Step=2/7460] loss=225.9497
24-Feb-25 16:20:15 - [Epoch=1 | Step=3/7460] loss=217.6718
24-Feb-25 16:20:16 - [Epoch=1 | Step=4/7460] loss=212.5616
24-Feb-25 16:20:16 - [Epoch=1 | Step=5/7460] loss=218.3426
24-Feb-25 16:20:16 - [Epoch=1 | Step=6/7460] loss=215.0799
24-Feb-25 16:20:17 - [Epoch=1 | Step=7/7460] loss=206.0480
24-Feb-25 16:20:17 - [Epoch=1 | Step=8/7460] loss=211.8779
24-Feb-25 16:20:17 - [Epoch=1 | Step=9/7460] loss=214.9130
24-Feb-25 16:20:18 - [Epoch=1 | Step=10/7460] loss=217.7435
24-Feb-25 16:20:18 - [Epoch=1 | Step=11/7460] loss=208.9541

24-Feb-25 16:21:34 - [Epoch=1 | Step=227/7460] loss=114.7441
24-Feb-25 16:21:34 - [Epoch=1 | Step=228/7460] loss=111.8840
24-Feb-25 16:21:35 - [Epoch=1 | Step=229/7460] loss=112.9706
24-Feb-25 16:21:35 - [Epoch=1 | Step=230/7460] loss=118.5322
24-Feb-25 16:21:36 - [Epoch=1 | Step=231/7460] loss=116.8485
24-Feb-25 16:21:36 - [Epoch=1 | Step=232/7460] loss=123.6357
24-Feb-25 16:21:36 - [Epoch=1 | Step=233/7460] loss=116.7761
24-Feb-25 16:21:37 - [Epoch=1 | Step=234/7460] loss=120.1632
24-Feb-25 16:21:37 - [Epoch=1 | Step=235/7460] loss=122.8251
24-Feb-25 16:21:37 - [Epoch=1 | Step=236/7460] loss=115.3569
24-Feb-25 16:21:38 - [Epoch=1 | Step=237/7460] loss=100.3841
24-Feb-25 16:21:38 - [Epoch=1 | Step=238/7460] loss=107.2543
24-Feb-25 16:21:39 - [Epoch=1 | Step=239/7460] loss=116.1457
24-Feb-25 16:21:39 - [Epoch=1 | Step=240/7460] loss=116.6003
24-Feb-25 16:21:39 - [Epoch=1 | Step=241/7460] loss=117.6250
24-Feb-25 16:21:40 - [Epoch=1 | Step=242/7460] loss=109.2654
24-Feb-25 16:21:40 - [Epoch=1 | Step=243/7460] loss=104.6087
24-Feb-25 16:21:40 - [Epoch=1 | Step=244/7460] loss=114.3172
24-Feb-25 16:21:41 - [Epoch=1 | Step=245/7460] loss=109.6914
24-Feb-25 16:21:41 - [Epoch=1 | Step=246/7460] loss=106.5784
24-Feb-25 16:21:41 - [Epoch=1 | Step=247/7460] loss=101.1286
24-Feb-25 16:21:42 - [Epoch=1 | Step=248/7460] loss=104.1650
24-Feb-25 16:21:42 - [Epoch=1 | Step=249/7460] loss=101.6536
24-Feb-25 16:21:42 - [Epoch=1 | Step=250/7460] loss=112.0337
24-Feb-25 16:21:43 - [Epoch=1 | Step=251/7460] loss=104.0924
24-Feb-25 16:21:43 - [Epoch=1 | Step=252/7460] loss=107.4064
24-Feb-25 16:21:44 - [Epoch=1 | Step=253/7460] loss=112.0278
24-Feb-25 16:21:44 - [Epoch=1 | Step=254/7460] loss=107.8160
24-Feb-25 16:21:44 - [Epoch=1 | Step=255/7460] loss=111.5178
24-Feb-25 16:21:45 - [Epoch=1 | Step=256/7460] loss=102.3740
24-Feb-25 16:21:45 - [Epoch=1 | Step=257/7460] loss=102.1553
24-Feb-25 16:21:45 - [Epoch=1 | Step=258/7460] loss=98.3380
24-Feb-25 16:21:46 - [Epoch=1 | Step=259/7460] loss=92.3927
24-Feb-25 16:21:46 - [Epoch=1 | Step=260/7460] loss=109.8945
24-Feb-25 16:21:46 - [Epoch=1 | Step=261/7460] loss=106.6463
24-Feb-25 16:21:47 - [Epoch=1 | Step=262/7460] loss=86.4954
24-Feb-25 16:21:47 - [Epoch=1 | Step=263/7460] loss=90.2110
24-Feb-25 16:21:47 - [Epoch=1 | Step=264/7460] loss=103.6239
24-Feb-25 16:21:48 - [Epoch=1 | Step=265/7460] loss=97.7140
24-Feb-25 16:21:48 - [Epoch=1 | Step=266/7460] loss=85.3189
24-Feb-25 16:21:48 - [Epoch=1 | Step=267/7460] loss=90.0862
24-Feb-25 16:21:49 - [Epoch=1 | Step=268/7460] loss=79.3944
24-Feb-25 16:21:49 - [Epoch=1 | Step=269/7460] loss=90.1115
24-Feb-25 16:21:50 - [Epoch=1 | Step=270/7460] loss=86.5530
24-Feb-25 16:21:50 - [Epoch=1 | Step=271/7460] loss=93.0245
24-Feb-25 16:21:50 - [Epoch=1 | Step=272/7460] loss=91.9504
24-Feb-25 16:21:51 - [Epoch=1 | Step=273/7460] loss=93.7486
24-Feb-25 16:21:51 - [Epoch=1 | Step=274/7460] loss=95.6950
24-Feb-25 16:21:51 - [Epoch=1 | Step=275/7460] loss=88.5654
24-Feb-25 16:21:52 - [Epoch=1 | Step=276/7460] loss=85.6634
24-Feb-25 16:21:52 - [Epoch=1 | Step=277/7460] loss=77.3680
24-Feb-25 16:21:52 - [Epoch=1 | Step=278/7460] loss=88.7183
24-Feb-25 16:21:53 - [Epoch=1 | Step=279/7460] loss=92.2577
24-Feb-25 16:21:53 - [Epoch=1 | Step=280/7460] loss=84.6005
24-Feb-25 16:21:53 - [Epoch=1 | Step=281/7460] loss=75.1318
24-Feb-25 16:21:54 - [Epoch=1 | Step=282/7460] loss=84.0627
24-Feb-25 16:21:54 - [Epoch=1 | Step=283/7460] loss=93.3766
24-Feb-25 16:21:54 - [Epoch=1 | Step=284/7460] loss=78.2684
24-Feb-25 16:21:55 - [Epoch=1 | Step=285/7460] loss=81.4327
24-Feb-25 16:21:55 - [Epoch=1 | Step=286/7460] loss=77.1527
24-Feb-25 16:21:56 - [Epoch=1 | Step=287/7460] loss=73.7762
24-Feb-25 16:21:56 - [Epoch=1 | Step=288/7460] loss=81.2791
24-Feb-25 16:21:56 - [Epoch=1 | Step=289/7460] loss=76.1266
24-Feb-25 16:21:57 - [Epoch=1 | Step=290/7460] loss=83.7769
24-Feb-25 16:21:57 - [Epoch=1 | Step=291/7460] loss=75.6856
24-Feb-25 16:21:57 - [Epoch=1 | Step=292/7460] loss=76.7764
24-Feb-25 16:21:58 - [Epoch=1 | Step=293/7460] loss=66.9956
24-Feb-25 16:21:58 - [Epoch=1 | Step=294/7460] loss=69.2103
24-Feb-25 16:21:58 - [Epoch=1 | Step=295/7460] loss=71.1524
24-Feb-25 16:21:59 - [Epoch=1 | Step=296/7460] loss=74.1762
24-Feb-25 16:21:59 - [Epoch=1 | Step=297/7460] loss=69.8568
24-Feb-25 16:21:59 - [Epoch=1 | Step=298/7460] loss=70.3490
24-Feb-25 16:22:00 - [Epoch=1 | Step=299/7460] loss=72.9074
24-Feb-25 16:22:00 - [Epoch=1 | Step=300/7460] loss=76.8217
24-Feb-25 16:22:00 - [Epoch=1 | Step=301/7460] loss=74.5255
24-Feb-25 16:22:01 - [Epoch=1 | Step=302/7460] loss=67.0900
24-Feb-25 16:22:01 - [Epoch=1 | Step=303/7460] loss=73.1765
24-Feb-25 16:22:01 - [Epoch=1 | Step=304/7460] loss=60.3462
24-Feb-25 16:22:02 - [Epoch=1 | Step=305/7460] loss=64.7396
24-Feb-25 16:22:02 - [Epoch=1 | Step=306/7460] loss=66.7640
24-Feb-25 16:22:02 - [Epoch=1 | Step=307/7460] loss=67.1423
24-Feb-25 16:22:03 - [Epoch=1 | Step=308/7460] loss=55.5533
24-Feb-25 16:22:03 - [Epoch=1 | Step=309/7460] loss=52.5845
24-Feb-25 16:22:03 - [Epoch=1 | Step=310/7460] loss=59.0087
24-Feb-25 16:22:04 - [Epoch=1 | Step=311/7460] loss=59.3765
24-Feb-25 16:22:04 - [Epoch=1 | Step=312/7460] loss=63.1074
24-Feb-25 16:22:04 - [Epoch=1 | Step=313/7460] loss=54.1321
24-Feb-25 16:22:05 - [Epoch=1 | Step=314/7460] loss=61.4910
24-Feb-25 16:22:05 - [Epoch=1 | Step=315/7460] loss=63.0210
24-Feb-25 16:22:05 - [Epoch=1 | Step=316/7460] loss=52.0115
24-Feb-25 16:22:06 - [Epoch=1 | Step=317/7460] loss=51.5250
24-Feb-25 16:22:06 - [Epoch=1 | Step=318/7460] loss=66.8444
24-Feb-25 16:22:07 - [Epoch=1 | Step=319/7460] loss=50.7378
24-Feb-25 16:22:07 - [Epoch=1 | Step=320/7460] loss=55.9059
24-Feb-25 16:22:07 - [Epoch=1 | Step=321/7460] loss=54.2387
24-Feb-25 16:22:08 - [Epoch=1 | Step=322/7460] loss=52.9752
24-Feb-25 16:22:08 - [Epoch=1 | Step=323/7460] loss=58.9905
24-Feb-25 16:22:08 - [Epoch=1 | Step=324/7460] loss=45.8326
24-Feb-25 16:22:09 - [Epoch=1 | Step=325/7460] loss=46.6769
24-Feb-25 16:22:09 - [Epoch=1 | Step=326/7460] loss=37.6015
24-Feb-25 16:22:09 - [Epoch=1 | Step=327/7460] loss=50.0750
24-Feb-25 16:22:10 - [Epoch=1 | Step=328/7460] loss=47.4972
24-Feb-25 16:22:10 - [Epoch=1 | Step=329/7460] loss=43.2746
24-Feb-25 16:22:10 - [Epoch=1 | Step=330/7460] loss=39.4485
24-Feb-25 16:22:11 - [Epoch=1 | Step=331/7460] loss=43.7084
24-Feb-25 16:22:11 - [Epoch=1 | Step=332/7460] loss=38.1991
24-Feb-25 16:22:11 - [Epoch=1 | Step=333/7460] loss=41.7080
24-Feb-25 16:22:12 - [Epoch=1 | Step=334/7460] loss=40.8708
24-Feb-25 16:22:12 - [Epoch=1 | Step=335/7460] loss=32.9418
24-Feb-25 16:22:13 - [Epoch=1 | Step=336/7460] loss=49.5360
24-Feb-25 16:22:13 - [Epoch=1 | Step=337/7460] loss=45.3889
24-Feb-25 16:22:13 - [Epoch=1 | Step=338/7460] loss=28.9515
24-Feb-25 16:22:14 - [Epoch=1 | Step=339/7460] loss=40.9712
24-Feb-25 16:22:14 - [Epoch=1 | Step=340/7460] loss=32.6817
24-Feb-25 16:22:14 - [Epoch=1 | Step=341/7460] loss=39.2722
24-Feb-25 16:22:15 - [Epoch=1 | Step=342/7460] loss=46.0405
24-Feb-25 16:22:15 - [Epoch=1 | Step=343/7460] loss=34.8678
24-Feb-25 16:22:15 - [Epoch=1 | Step=344/7460] loss=38.6186
24-Feb-25 16:22:16 - [Epoch=1 | Step=345/7460] loss=36.7691
24-Feb-25 16:22:16 - [Epoch=1 | Step=346/7460] loss=29.5933
24-Feb-25 16:22:16 - [Epoch=1 | Step=347/7460] loss=34.9836
24-Feb-25 16:22:17 - [Epoch=1 | Step=348/7460] loss=34.6896
24-Feb-25 16:22:17 - [Epoch=1 | Step=349/7460] loss=27.3362
24-Feb-25 16:22:17 - [Epoch=1 | Step=350/7460] loss=32.0600

```

think deeply what could be the problem

pos encoding is like this and learned

```
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len):
        super().__init__()
        self.pe = nn.Embedding(max_len, d_model)
        self.max_len = max_len

    def forward(self, x):
        """x shape: (batch_size, seq_len, d_model)"""
        batch_size, seq_len, _ = x.size()
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)  # (1, seq_len)
        pe = self.pe(positions)  # (1, seq_len, d_model)
        return x + pe
```

think deeply what could be the problem, why loss is plateauing