"""
Using torch.MultiHeadAttention instead of custom implementation
"""

# !pip install datasets
# !pip install --upgrade sentencepiece

# configure logging
import torch.nn.functional as F
import torch.nn as nn
import torch
import sentencepiece as spm
from torch.utils.data import  DataLoader,SubsetRandomSampler
import glob
import os
import numpy as np
from dataset import PathLossDataset
import math
import numpy as np
import logging as log
import os
import gc
import pandas as pd
import glob
from datetime import datetime


datetimesatmp = datetime.now().strftime("%Y%m%d%H%M%S")

outfile = f"./logs/pl_{datetimesatmp}_.log"
log.basicConfig(level=log.INFO,
                format='%(asctime)s - %(message)s',
                datefmt='%d-%b-%y %H:%M:%S',
                handlers=[
                    log.FileHandler(outfile),
                    log.StreamHandler(),
                ],
                force=True,
                )

loss_log_file_base = f"./logs/loss_log_{datetimesatmp}_.npy"
loss_log_file = f"./logs/loss_log_{datetimesatmp}_.npy.npz"

# Initialize loss log
if not os.path.exists(loss_log_file):
    np.savez_compressed(loss_log_file, loss=np.array([]))
    log.info(f"Created path loss file {loss_log_file}")


# we need to add positional encoding to the input_ids
# Positional encoding is a way to provide the model with information about the position of each token in the sequence.
# This is important because the model has no inherent sense of order in the tokens, since it only sees them as embeddings.
# generated by LLM
class PositionalEncoding(nn.Module):
    def __init__(self, d_model,max_len=768):
        super().__init__()


        # Create positional encoding matrix [max_len, d_model]
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # Shape: [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # Sinusoidal encoding (only needed if d_model > 1, so avoid it here)
        pe[:, 0::2] = torch.sin(position * div_term) if d_model > 1 else position / max_len  # Simple scaling
        pe[:, 1::2] = torch.cos(position * div_term) if d_model > 1 else 0  # Avoid unused dim

        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]

        # Register as buffer (so it's automatically moved with model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        x shape: (batch_size, seq_len)
        We add positional encoding up to seq_len from the precomputed 'pe'.
        """
        seq_len = x.size(1)  #  Extract sequence length
        #  Expand x to match `d_model = 1` for element-wise addition
        x = x.unsqueeze(-1)  # Now (batch_size, seq_len, 1)
        # Ensure correct broadcasting: self.pe is (1, max_len, 1), slicing works
        return x + self.pe[:, :seq_len, :]  # Correctly adds positional encoding
# Instead of doing Multi head sequentially like previous, lets do it in parallel

vocab_size = 2000
d_k = 64  # attention size
read_seq_length = 768
d_model = 512  # embediding size
final_size = 1 # we just want one value
num_heads = 8  
extra_feature_size = 4
# add in the embdeiing part from previous layer
pos_encoding = PositionalEncoding(d_model,read_seq_length)
extra_features_embedding = nn.Linear(extra_feature_size, d_model)
multihead_attention = nn.MultiheadAttention(d_model, num_heads, dropout=0.1,batch_first=True)
prediction_layer1 = nn.Linear(d_model, vocab_size)
layer_norm1 = nn.LayerNorm(vocab_size)
prediction_layer2 = nn.Linear(vocab_size, final_size)
layer_norm2 = nn.LayerNorm(final_size)  # last dimension is the vocab size
# Define the loss function
loss_function = nn.SmoothL1Loss() # since we have just regression
# We'll combine these into a simple pipeline
model = nn.ModuleList([pos_encoding,
                      multihead_attention, extra_features_embedding,
                      layer_norm1, layer_norm2,
                      prediction_layer1, prediction_layer2])
# SGD is unstable and hence we use this
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# with higher learning loss is Nan

# Place all in GPU
pos_encoding.to('cuda')
multihead_attention.to('cuda')
extra_features_embedding.to('cuda')
layer_norm1.to('cuda')
layer_norm2.to('cuda')
prediction_layer1.to('cuda')
prediction_layer2.to('cuda')
model.to('cuda')
# NO NEED TO EXECUTE THIS AGAIN ( this need A100, )
log.info("Training model...")

BATCH_SIZE = 25  # 5 GB for 
model.train()
loss_value_list = []

# Define the folder containing Parquet files
INPUT_DIR = "loss_parquet_files"
# Get a list of all Parquet files in the folder (sorted for consistency)
parquet_files = sorted(glob.glob(os.path.join(INPUT_DIR, "*.parquet")))
nfiles = len(parquet_files)
print(f"Number of parquet_files= {nfiles}")

parquet_files = parquet_files[:10]  # Limit to 10 files for testing

dataset = PathLossDataset(parquet_files)
dataset_len = len(dataset)
train_ratio = 0.8
train_len = int(dataset_len * train_ratio)
val_len = dataset_len - train_len

indices = list(range(dataset_len))
np.random.shuffle(indices)  # Shuffle the indices

train_indices = indices[:train_len]
val_indices = indices[train_len:]

train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=4, pin_memory=True)
val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=4, pin_memory=True)

for epoch in range(2):
    model.train()
    epoch_loss = 0.0
    num_batches = 0
    for i, (input_features, elevation_data, path_loss) in enumerate(train_loader):
        #print(f"Batch {i}: Extra features shape: {extra_features.shape}, Elevation data shape: {elevation_data.shape}, Path loss shape: {path_loss.shape}")
        step = i + 1
        # Move to GPU
        input_features = input_features.to('cuda')
        elevation_data = elevation_data.to('cuda')
        target_labels = path_loss.to('cuda')

        # project to higher dim
        extra_features_tokens = extra_features_embedding(input_features)
        pos_embedded_tokens = pos_encoding(elevation_data) 
        score,_ = multihead_attention(pos_embedded_tokens,pos_embedded_tokens,pos_embedded_tokens)
        hidden1 = score +pos_embedded_tokens
        pooled_score = hidden1.mean(dim=1)  # ([20, 512])
        hidden2 = prediction_layer1(pooled_score)  # through few linear layers
        hidden2 = layer_norm1(hidden2) 
        logits =  prediction_layer2(hidden2)      # add layer norm
        loss = loss_function(
            logits.squeeze(1),
            target_labels
        )
        optimizer.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        num_batches += 1

          # We are not discarding the loss or ignoring it; rather, weâ€™re enforcing a limit on the size of the update to avoid erratic jumps.
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        optimizer.step()
        if epoch == 0 and step == 1:
            log.info(f"Input Features Shape: {input_features.shape}")  # Expected: (BATCH_SIZE, 4)
            log.info(f"Elevation Data Shape: {elevation_data.shape}")  # Expected: (BATCH_SIZE, read_seq_length)
            log.info(f"Path Loss Shape: {path_loss.shape}")  # Expected: (BATCH_SIZE,)
            log.info(f"extra_features_tokens.shape {extra_features_tokens.shape}") #([20, 512])
            log.info(f"Elevation Data Shape: {elevation_data.shape}") # ([20, 768])
            log.info(f"pos_embedded_tokens.shape {pos_embedded_tokens.shape}") #[20, 768, 512])
            log.info(f"score.shape {score.shape}") # [20, 768, 512]
            log.info(f"hidden1.shape {hidden1.shape}")
            log.info(f"pooled_score.shape {pooled_score.shape}")
            log.info(f"hidden2.shape {hidden2.shape}")
            log.info(f"logits.shape {logits.shape}")
        # print training progress occasionally
        loss_value_list.append((epoch, step, loss.item()))
        if step % 100 == 0:
            log.info("[Epoch=%d | Step=%d/%d] loss=%.4f",
                     epoch+1, step, total_steps,loss.item())
            data = np.load(loss_log_file,allow_pickle=True)
            loss_history = []
            if "loss" in data:
                # Convert to list for appending
                loss_history = data["loss"].tolist()
            loss_list = loss_history + loss_value_list
            np.savez_compressed(
                loss_log_file, loss=np.array(loss_list, dtype=object))
            loss_value_list = []
        del  input_features,elevation_data,target_labels 
        gc.collect()
        torch.cuda.empty_cache()
    avg_epoch_loss = epoch_loss / num_batches
    log.info("---------Epoch %02d | Average Loss: %.4f", epoch+1, avg_epoch_loss)
    # do a validation loss

    model.eval()
    validation_loss = 0
    num_valid_batches = 0
    overestimation_count = 0 # like false positive
    underestimation_count = 0 # like false negative
    for i, (input_features, elevation_data, path_loss) in enumerate(val_loader):
  
        # Move to GPU
        input_features = input_features.to('cuda')
        elevation_data = elevation_data.to('cuda')
        target_labels = path_loss.to('cuda')

        # project to higher dim
        extra_features_tokens = extra_features_embedding(input_features)
        pos_embedded_tokens = pos_encoding(elevation_data) 
        score,_ = multihead_attention(pos_embedded_tokens,pos_embedded_tokens,pos_embedded_tokens)
        hidden1 = score +pos_embedded_tokens
        pooled_score = hidden1.mean(dim=1)  # ([20, 512])
        hidden2 = prediction_layer1(pooled_score)  # through few linear layers
        hidden2 = layer_norm1(hidden2) 
        logits =  prediction_layer2(hidden2)      # add layer norm
        loss = loss_function(
            logits.squeeze(1),
            target_labels
        )
        num_valid_batches += 1
        validation_loss += loss.item()
        # Calculate overestimation and underestimation counts
        predictions = logits.squeeze(1).cpu().detach().numpy()
        targets = target_labels.cpu().detach().numpy()
        diff = predictions - targets
        overestimation_count += np.sum(diff > 0.2)
        underestimation_count += np.sum(diff < 0.2)
    avg_valid_loss = validation_loss / num_valid_batches
    
    log.info("---------Epoch %02d | Average Validation : %.4f", epoch+1, avg_valid_loss)
    log.info(f"---------Epoch %02d | Overestimation Count (like False Positive): {overestimation_count}", epoch+1)
    log.info(f"---------Epoch %02d | Underestimation Count (like False Negative): {underestimation_count}", epoch+1)

"""# Use the trained model to predict"""

# save the model weights
# add data to the model

save_path = f"./weights/model_weights{datetimesatmp}.pth"
torch.save(model.state_dict(), save_path)
log.info(f"Model weights saved at {save_path}")


log.info(f"Training Over")

