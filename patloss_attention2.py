"""
Using torch.MultiHeadAttention instead of custom implementation
"""

# !pip install datasets
# !pip install --upgrade sentencepiece

# configure logging
import torch.nn.functional as F
import torch.nn as nn
import torch
import sentencepiece as spm
from torch.utils.data import  DataLoader,SubsetRandomSampler
import glob
import os
import numpy as np
from dataset import PathLossDataset
import math
import numpy as np
import logging as log
import os
import gc
import pandas as pd
import glob
from datetime import datetime


datetimesatmp = datetime.now().strftime("%Y%m%d%H%M%S")

outfile = f"./logs/pl_{datetimesatmp}_.log"
#create logs directory if it does not exist
if not os.path.exists("logs"):
    os.makedirs("logs")
if not os.path.exists("weights"):
    os.makedirs("weights")
    
log.basicConfig(level=log.INFO,
                format='%(asctime)s - %(message)s',
                datefmt='%d-%b-%y %H:%M:%S',
                handlers=[
                    log.FileHandler(outfile),
                    log.StreamHandler(),
                ],
                force=True,
                )

loss_log_file_base = f"./logs/loss_log_{datetimesatmp}_.npy"
loss_log_file = f"./logs/loss_log_{datetimesatmp}_.npy.npz"

# Initialize loss log
if not os.path.exists(loss_log_file):
    np.savez_compressed(loss_log_file, loss=np.array([]))
    log.info(f"Created path loss file {loss_log_file}")


# we need to add positional encoding to the input_ids
# Positional encoding is a way to provide the model with information about the position of each token in the sequence.
# This is important because the model has no inherent sense of order in the tokens, since it only sees them as embeddings.
# generated by LLM
class PositionalEncodingSinuSoidal(nn.Module):
    def __init__(self, d_model, max_len):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        """x shape: (batch_size, seq_len, d_model)"""
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]  # Directly add to 3D input   
    
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len):
        super().__init__()
        self.pe = nn.Embedding(max_len, d_model)
        self.max_len = max_len

    def forward(self, x):
        """x shape: (batch_size, seq_len, d_model)"""
        batch_size, seq_len, _ = x.size()
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)  # (1, seq_len)
        pe = self.pe(positions)  # (1, seq_len, d_model)
        return x + pe
# Instead of doing Multi head sequentially like previous, lets do it in parallel

# Replace ModuleList with Sequential:
# class TransformerBlock(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
#         self.ffn = nn.Sequential(
#             nn.Linear(d_model, 4*d_model),
#             nn.ReLU(),
#             nn.Linear(4*d_model, d_model)
#         )
#         self.norm1 = nn.LayerNorm(d_model)
#         self.norm2 = nn.LayerNorm(d_model)
    
#     def forward(self, x):
#         x = self.norm1(x + self.attn(x, x, x)[0])
#         x = self.norm2(x + self.ffn(x))
#         return x
# model = nn.Sequential(
#     TransformerBlock(),
#     TransformerBlock(),
#     nn.Linear(d_model, 1)
# )


seq_length = 765
d_model = 160  # embediding size
final_size = 1 # we just want one value
num_heads = 16  
extra_feature_size = 4

pos_encoding = PositionalEncoding(d_model,seq_length)
input_features_projection = nn.Linear(extra_feature_size, d_model)
elevation_projection = nn.Linear(1, d_model)  # Add this to model
multihead_attention = nn.MultiheadAttention(d_model, num_heads, dropout=0.1,batch_first=True)
# multihead_attention = nn.TransformerEncoderLayer(
#     d_model=d_model,
#     nhead=num_heads,
#     dim_feedforward=256,   # or bigger
#     dropout=0.1,
#     batch_first=True,
# )
prediction_layer1 = nn.Linear(d_model, d_model)
layer_norm1 = nn.LayerNorm(d_model)
prediction_layer2 = nn.Linear(d_model, d_model*4)
layer_norm2 = nn.LayerNorm(d_model*4) 
prediction_layer3 = nn.Linear(d_model*4, final_size)

# Define the loss function
loss_function = nn.SmoothL1Loss() # since we have just regression
# We'll combine these into a simple pipeline
model = nn.ModuleList([input_features_projection,elevation_projection,pos_encoding,
                      multihead_attention, prediction_layer1,
                      layer_norm1, prediction_layer2,layer_norm2,prediction_layer3])
# SGD is unstable and hence we use this
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
# Set up a learning rate scheduler that reduces the LR when the loss plateaus.
# Here, we reduce the LR by a factor of 0.5 if there's no improvement for 5 epochs.
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5, verbose=True
)

# Place all in GPU
elevation_projection = elevation_projection.to('cuda')
pos_encoding.to('cuda')
multihead_attention.to('cuda')
input_features_projection.to('cuda')
layer_norm1.to('cuda')
layer_norm2.to('cuda')
prediction_layer1.to('cuda')
prediction_layer2.to('cuda')
model.to('cuda')
log.info("Training model...")

BATCH_SIZE = 25  # 5 GB for 
model.train()
loss_value_list = []

# Define the folder containing Parquet files
INPUT_DIR = "loss_parquet_files"
# Get a list of all Parquet files in the folder (sorted for consistency)
parquet_files = sorted(glob.glob(os.path.join(INPUT_DIR, "*.parquet")))
nfiles = len(parquet_files)
print(f"Number of parquet_files= {nfiles}")

parquet_files = parquet_files[:1000]  # Limit to 10 files for testing

dataset = PathLossDataset(parquet_files)
dataset_len = len(dataset)
train_ratio = 0.9
train_len = int(dataset_len * train_ratio)
val_len = dataset_len - train_len

indices = list(range(dataset_len))
np.random.shuffle(indices)  # Shuffle the indices

train_indices = indices[:train_len]
val_indices = indices[train_len:]

train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=4, pin_memory=True)
val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=4, pin_memory=True)

# Calculate total steps for training and validation
train_steps_per_epoch = len(train_loader)
val_steps_per_epoch = len(val_loader)

log.info(f"Training steps per epoch: {train_steps_per_epoch}")
log.info(f"Validation steps per epoch: {val_steps_per_epoch}")

def forward_pass(input_features, elevation_data, target_labels, padding_mask):
    input_features = input_features.to('cuda') # (B, 4)
    elevation_data = elevation_data.to('cuda')  # (B, 765) = (B, S)
    target_labels = target_labels.to('cuda') # (B, 1)
    padding_mask = padding_mask.to('cuda')
    # project to higher dim
    #print(f"input_features.shape {input_features.shape}") #torch.Size([25, 4])
    other_features_embed = input_features_projection(input_features) # (B, d_model)
   # print(f"other_features_embed.shape {other_features_embed.shape}") #t (B, d_model)
    elevation_embed = elevation_projection(elevation_data.unsqueeze(-1))  
    #print(f"elevation_embed.shape {elevation_embed.shape}") # (B, S, d_model)
    pos_embed = pos_encoding(elevation_embed) 
    #print(f"pos_embed.shape {pos_embed.shape}") # (B, S, d_model)
    elevation_score, _ = multihead_attention(pos_embed, pos_embed, pos_embed)#,key_padding_mask=padding_mask)
    #elevation_score = multihead_attention(pos_embed)# Transformer Encoder one
    # pool over S
    elevation_vector = torch.mean(elevation_score, dim=1)  # (B, d_model)
    #print   (f"Elevation vector shape: {elevation_vector.shape}")
    combined = other_features_embed + elevation_vector  # (B, d_model)
    #print(f"Combined shape: {combined.shape}")
    hidden1 = F.relu(prediction_layer1(combined))
    hidden2 = layer_norm1(hidden1)
    hidden2 = F.relu(prediction_layer2(hidden2))
    hidden3 = layer_norm2(hidden2)
    logits  = prediction_layer3(hidden3)
    # print(f"Logits shape: {logits.shape}") # (B, 1)
    logits = logits.squeeze(-1)  
    loss = loss_function(
            logits,
            target_labels
        )
    #del  input_features,elevation_data,target_labels 
    #gc.collect()
    #torch.cuda.empty_cache()
    return logits,loss

for epoch in range(1):
    model.train()
    epoch_loss = 0.0
    num_batches = 0
    step_loss =0
    for i, (input_features, elevation_data, target_labels,padding_mask) in enumerate(train_loader):
        #print(f"Batch {i}: Extra features shape: {extra_features.shape}, Elevation data shape: {elevation_data.shape}, Path loss shape: {path_loss.shape}")
        step = i + 1
        # Add data validation checks
        # Move to GPU
        logits, loss = forward_pass(input_features, elevation_data, target_labels, padding_mask)
        optimizer.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        step_loss += loss.item()
        num_batches += 1

          # We are not discarding the loss or ignoring it; rather, weâ€™re enforcing a limit on the size of the update to avoid erratic jumps.
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        loss_value_list.append((epoch, step, loss.item()))
        log.info("[Epoch=%d | Step=%d/%d] loss=%.4f",
                     epoch+1, step, train_steps_per_epoch,loss.item())
        if step % 500 == 0:
            data = np.load(loss_log_file,allow_pickle=True)
            loss_history = []
            if "loss" in data:
                # Convert to list for appending
                loss_history = data["loss"].tolist()
            loss_list = loss_history + loss_value_list
            np.savez_compressed(
                loss_log_file, loss=np.array(loss_list, dtype=object))
            loss_value_list = []
        if step % 1000 == 0:
            # For ReduceLROnPlateau, call scheduler.step() with the average loss
            avg_loss = step_loss / 500
            step_loss =0
            scheduler.step(avg_loss)
            current_lr = optimizer.param_groups[0]['lr']
            log.info(f"Epoch {epoch+1}/{step}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}")
    

    avg_epoch_loss = epoch_loss / num_batches
    log.info("---------Epoch %02d | Average Loss: %.4f", epoch+1, avg_epoch_loss)
    save_path = f"./weights/model_weights{datetimesatmp}.pth"
    torch.save(model.state_dict(), save_path)
    log.info(f"Model weights saved at {save_path}")
    log.info(f"Training Loss saved at {loss_log_file}")
    # do a validation loss

    model.eval()
    validation_loss = 0
    num_valid_batches = 0
    overestimation_count = 0 # like false positive
    underestimation_count = 0 # like false negative
    for i, (input_features, elevation_data, target_labels,padding_mask) in enumerate(val_loader):
        logits,loss= forward_pass(input_features, elevation_data, target_labels, padding_mask)
        num_valid_batches += 1
        validation_loss += loss.item()
        # Calculate overestimation and underestimation counts
        predictions = logits.cpu().detach().numpy()
        targets = target_labels.cpu().detach().numpy()
        diff = predictions - targets
        overestimation_count += np.sum(diff > 0.5)
        underestimation_count += np.sum(diff < 0.5)
    avg_valid_loss = validation_loss / num_valid_batches
    
    log.info("---------Epoch %02d | Average Validation : %.4f", epoch+1, avg_valid_loss)
    log.info(f"---------Epoch %02d | Overestimation Count (like False Positive): {overestimation_count}", epoch+1)
    log.info(f"---------Epoch %02d | Underestimation Count (like False Negative): {underestimation_count}", epoch+1)

"""# Use the trained model to predict"""

# save the model weights
# add data to the model

save_path = f"./weights/model_weights{datetimesatmp}.pth"
torch.save(model.state_dict(), save_path)
log.info(f"Model weights saved at {save_path}")
log.info(f"Training Loss saved at {loss_log_file}")

log.info(f"Training Over")

