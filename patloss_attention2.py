"""
Using torch.MultiHeadAttention instead of custom implementation
"""

# !pip install datasets
# !pip install --upgrade sentencepiece

# configure logging
import torch.nn.functional as F
import torch.nn as nn
import torch
import sentencepiece as spm
from torch.utils.data import  DataLoader,SubsetRandomSampler
import glob
import os
import numpy as np
from dataset import PathLossDataset
import math
import numpy as np
import logging as log
import os
import gc
import pandas as pd
import glob
from datetime import datetime


datetimesatmp = datetime.now().strftime("%Y%m%d%H%M%S")

outfile = f"./logs/pl_{datetimesatmp}_.log"
#create logs directory if it does not exist
if not os.path.exists("logs"):
    os.makedirs("logs")
if not os.path.exists("weights"):
    os.makedirs("weights")
    
log.basicConfig(level=log.INFO,
                format='%(asctime)s - %(message)s',
                datefmt='%d-%b-%y %H:%M:%S',
                handlers=[
                    log.FileHandler(outfile),
                    log.StreamHandler(),
                ],
                force=True,
                )

loss_log_file_base = f"./logs/loss_log_{datetimesatmp}_.npy"
loss_log_file = f"./logs/loss_log_{datetimesatmp}_.npy.npz"

# Initialize loss log
if not os.path.exists(loss_log_file):
    np.savez_compressed(loss_log_file, loss=np.array([]))
    log.info(f"Created path loss file {loss_log_file}")


# we need to add positional encoding to the input_ids
# Positional encoding is a way to provide the model with information about the position of each token in the sequence.
# This is important because the model has no inherent sense of order in the tokens, since it only sees them as embeddings.
# generated by LLM
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=768):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        """x shape: (batch_size, seq_len, d_model)"""
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]  # Directly add to 3D input   
    
# Instead of doing Multi head sequentially like previous, lets do it in parallel

# Replace ModuleList with Sequential:
# class TransformerBlock(nn.Module):
#     def __init__(self):
#         super().__init__()
#         self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
#         self.ffn = nn.Sequential(
#             nn.Linear(d_model, 4*d_model),
#             nn.ReLU(),
#             nn.Linear(4*d_model, d_model)
#         )
#         self.norm1 = nn.LayerNorm(d_model)
#         self.norm2 = nn.LayerNorm(d_model)
    
#     def forward(self, x):
#         x = self.norm1(x + self.attn(x, x, x)[0])
#         x = self.norm2(x + self.ffn(x))
#         return x
# model = nn.Sequential(
#     TransformerBlock(),
#     TransformerBlock(),
#     nn.Linear(d_model, 1)
# )


vocab_size = 2000
d_k = 64  # attention size
read_seq_length = 765
d_model = 512  # embediding size
final_size = 1 # we just want one value
num_heads = 8  
extra_feature_size = 4

pos_encoding = PositionalEncoding(d_model,read_seq_length)
extra_features_embedding = nn.Linear(extra_feature_size, d_model)
multihead_attention = nn.MultiheadAttention(d_model, num_heads, dropout=0.1,batch_first=True)
prediction_layer1 = nn.Linear(d_model, vocab_size)
layer_norm1 = nn.LayerNorm(vocab_size)
prediction_layer2 = nn.Linear(vocab_size, final_size)
layer_norm2 = nn.LayerNorm(final_size)  # last dimension is the vocab size
elevation_projection = nn.Linear(1, d_model)  # Add this to model

# Define the loss function
loss_function = nn.SmoothL1Loss() # since we have just regression
# We'll combine these into a simple pipeline
model = nn.Sequential(elevation_projection,pos_encoding,
                      multihead_attention, extra_features_embedding,
                      layer_norm1, layer_norm2,
                      prediction_layer1, prediction_layer2)
# SGD is unstable and hence we use this
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# with higher learning loss is Nan

# Place all in GPU
elevation_projection = elevation_projection.to('cuda')
pos_encoding.to('cuda')
multihead_attention.to('cuda')
extra_features_embedding.to('cuda')
layer_norm1.to('cuda')
layer_norm2.to('cuda')
prediction_layer1.to('cuda')
prediction_layer2.to('cuda')
model.to('cuda')
log.info("Training model...")

BATCH_SIZE = 25  # 5 GB for 
model.train()
loss_value_list = []

# Define the folder containing Parquet files
INPUT_DIR = "loss_parquet_files"
# Get a list of all Parquet files in the folder (sorted for consistency)
parquet_files = sorted(glob.glob(os.path.join(INPUT_DIR, "*.parquet")))
nfiles = len(parquet_files)
print(f"Number of parquet_files= {nfiles}")

parquet_files = parquet_files[:10]  # Limit to 10 files for testing

dataset = PathLossDataset(parquet_files)
dataset_len = len(dataset)
train_ratio = 0.8
train_len = int(dataset_len * train_ratio)
val_len = dataset_len - train_len

indices = list(range(dataset_len))
np.random.shuffle(indices)  # Shuffle the indices

train_indices = indices[:train_len]
val_indices = indices[train_len:]

train_sampler = SubsetRandomSampler(train_indices)
val_sampler = SubsetRandomSampler(val_indices)

train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=4, pin_memory=True)
val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=4, pin_memory=True)

# Calculate total steps for training and validation
train_steps_per_epoch = len(train_loader)
val_steps_per_epoch = len(val_loader)

log.info(f"Training steps per epoch: {train_steps_per_epoch}")
log.info(f"Validation steps per epoch: {val_steps_per_epoch}")

def forward_pass(input_features, elevation_data, target_labels, padding_mask):
    # input_features = input_features.to('cuda')
    # elevation_data = elevation_data.to('cuda')
    # target_labels = target_labels.to('cuda')
    # padding_mask = padding_mask.to('cuda')
        # project to higher dim
    extra_features_tokens = extra_features_embedding(input_features)
        # Replace elevation_data processing with:
    elevation_embed = elevation_projection(elevation_data.unsqueeze(-1))  # (B, S, d_model)
        #print(f"elevation_embed.shape {elevation_embed.shape}") torch.Size([25, 765, 512])
    pos_embedded_tokens = pos_encoding(elevation_embed) 
        #print(f"pos_embedded_tokens.shape {pos_embedded_tokens.shape}")  torch.Size([25, 765, 512])
        #print(f"extra_features_tokens.shape {extra_features_tokens.shape}") torch.Size([25, 512])
    combined = pos_embedded_tokens + extra_features_tokens.unsqueeze(1)  # [25, 1, 512] # (B, S, d_model)
    score, _ = multihead_attention(combined, combined, combined,key_padding_mask=padding_mask)
    hidden1 = score +pos_embedded_tokens
    pooled_score = hidden1.mean(dim=1)  # ([20, 512])
    hidden2 = prediction_layer1(pooled_score)  # through few linear layers
    hidden2 = layer_norm1(hidden2) 
    logits =  prediction_layer2(hidden2)      # add layer norm
    loss = loss_function(
            logits.squeeze(1),
            target_labels
        )
    del  input_features,elevation_data,target_labels 
    gc.collect()
    torch.cuda.empty_cache()
    return logits,loss

for epoch in range(1):
    model.train()
    epoch_loss = 0.0
    num_batches = 0
    for i, (input_features, elevation_data, target_labels,padding_mask) in enumerate(train_loader):
        #print(f"Batch {i}: Extra features shape: {extra_features.shape}, Elevation data shape: {elevation_data.shape}, Path loss shape: {path_loss.shape}")
        step = i + 1
        # Add data validation checks
        # Move to GPU
        logits, loss = forward_pass(input_features, elevation_data, target_labels, padding_mask)
        optimizer.zero_grad()
        loss.backward()
        epoch_loss += loss.item()
        num_batches += 1

          # We are not discarding the loss or ignoring it; rather, weâ€™re enforcing a limit on the size of the update to avoid erratic jumps.
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
        optimizer.step()
        loss_value_list.append((epoch, step, loss.item()))
        if step % 1 == 0:
            log.info("[Epoch=%d | Step=%d/%d] loss=%.4f",
                     epoch+1, step, train_steps_per_epoch,loss.item())
            data = np.load(loss_log_file,allow_pickle=True)
            loss_history = []
            if "loss" in data:
                # Convert to list for appending
                loss_history = data["loss"].tolist()
            loss_list = loss_history + loss_value_list
            np.savez_compressed(
                loss_log_file, loss=np.array(loss_list, dtype=object))
            loss_value_list = []

    avg_epoch_loss = epoch_loss / num_batches
    log.info("---------Epoch %02d | Average Loss: %.4f", epoch+1, avg_epoch_loss)
    # do a validation loss

    model.eval()
    validation_loss = 0
    num_valid_batches = 0
    overestimation_count = 0 # like false positive
    underestimation_count = 0 # like false negative
    for i, (input_features, elevation_data, target_labels,padding_mask) in enumerate(val_loader):
  
        logits,loss= forward_pass(input_features, elevation_data, target_labels, padding_mask)
        num_valid_batches += 1
        validation_loss += loss.item()
        # Calculate overestimation and underestimation counts
        predictions = logits.squeeze(1).cpu().detach().numpy()
        targets = target_labels.cpu().detach().numpy()
        diff = predictions - targets
        overestimation_count += np.sum(diff > 0.2)
        underestimation_count += np.sum(diff < 0.2)
    avg_valid_loss = validation_loss / num_valid_batches
    
    log.info("---------Epoch %02d | Average Validation : %.4f", epoch+1, avg_valid_loss)
    log.info(f"---------Epoch %02d | Overestimation Count (like False Positive): {overestimation_count}", epoch+1)
    log.info(f"---------Epoch %02d | Underestimation Count (like False Negative): {underestimation_count}", epoch+1)

"""# Use the trained model to predict"""

# save the model weights
# add data to the model

save_path = f"./weights/model_weights{datetimesatmp}.pth"
torch.save(model.state_dict(), save_path)
log.info(f"Model weights saved at {save_path}")


log.info(f"Training Over")

